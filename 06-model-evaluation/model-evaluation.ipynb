{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model evaluation** is a crucial step in the machine learning pipeline. It involves assessing the performance of a machine learning model to ensure it generalizes well to unseen data. Evaluation helps us understand how well our model is performing and provides insights into how we can improve it.\n",
    "\n",
    "### Why Do We Need Model Evaluation?\n",
    "\n",
    "1. **Performance Assessment**: To determine how well our model performs on new, unseen data.\n",
    "2. **Avoid Overfitting**: To ensure the model generalizes well and isn't just memorizing the training data.\n",
    "3. **Parameter Tuning**: To find the optimal settings for our model to achieve the best performance.\n",
    "4. **Comparison**: To compare different models and select the best one for our problem.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Load and Explore the Dataset**: Understand the structure and content of the Iris dataset.\n",
    "2. **Preprocess the Data**: Split the data into training and testing sets to prepare for model training and evaluation.\n",
    "3. **Define a Model**: Choose a machine learning model to evaluate—in this case, a Support Vector Machine (SVM).\n",
    "4. **Perform Cross-Validation**: Assess the model's performance through cross-validation to check its robustness and reliability.\n",
    "5. **Perform Grid Search for Hyperparameter Tuning**: Use grid search to find the best hyperparameters for our model to improve its performance.\n",
    "6. **Evaluate the Model Using Metrics**: Calculate various performance metrics to quantify how well the model is performing on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = [target_names[i] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "\n",
       "  species  \n",
       "0  setosa  \n",
       "1  setosa  \n",
       "2  setosa  \n",
       "3  setosa  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation** is a method used to evaluate the performance of a machine learning model by splitting the data into multiple parts, training the model on some parts, and testing it on others. It helps ensure that our model is reliable and performs well on new, unseen data.\n",
    "\n",
    "### What is Cross-Validation?\n",
    "\n",
    "Imagine we have a dataset and we want to check how well our machine learning model works. Instead of just testing the model on a single set of data, cross-validation helps us test it in a more thorough way by splitting the data into different parts and repeatedly training and testing the model.\n",
    "\n",
    "### How Cross-Validation Works\n",
    "\n",
    "1. **Split the Data**:\n",
    "   - We divide our dataset into several smaller parts (called \"folds\"). For example, if we use 5-fold cross-validation, we split the data into 5 parts.\n",
    "\n",
    "2. **Train and Test**:\n",
    "   - For each fold:\n",
    "     - **Train**: We use some of the folds to train the model.\n",
    "     - **Test**: We test the model on the remaining fold that wasn't used for training.\n",
    "     - We repeat this process for each fold so that every part of the data is used for both training and testing.\n",
    "\n",
    "3. **Evaluate Performance**:\n",
    "   - After testing the model on all folds, we will have several performance scores (like accuracy). We average these scores to get a better idea of how well the model performs overall.\n",
    "\n",
    "### Example with Iris Dataset and SVM\n",
    "\n",
    "Let’s say we have the Iris dataset and we want to use a Support Vector Machine (SVM) to classify the flowers into different species. Here’s how we would use cross-validation to evaluate the SVM model:\n",
    "\n",
    "1. **Split the Dataset**:\n",
    "   - We use 5-fold cross-validation. This means we split the Iris dataset into 5 parts.\n",
    "\n",
    "2. **Training and Testing**:\n",
    "   - For each of the 5 folds:\n",
    "     - **Train**: We use 4 of the folds to train the SVM model.\n",
    "     - **Test**: We test the model on the remaining 1 fold.\n",
    "     - We repeat this process 5 times, so each fold gets used for testing once.\n",
    "\n",
    "3. **Calculate Average Performance**:\n",
    "   - After completing the 5 folds, we will have 5 accuracy scores (one for each test fold). We average these scores to get an overall measure of how well the SVM model performs.\n",
    "\n",
    "### Why Use Cross-Validation?\n",
    "\n",
    "- **Reliable Evaluation**: It gives us a more reliable estimate of model performance because it uses multiple subsets of the data for both training and testing.\n",
    "- **Reduces Overfitting**: It helps ensure that the model doesn’t just perform well on one particular subset of the data but generalizes well across different parts of the dataset.\n",
    "\n",
    "\n",
    "\n",
    "Cross-validation is a technique to evaluate how well a machine learning model performs by splitting the data into multiple parts, training the model on some of these parts, and testing it on others. For example, using 5-fold cross-validation with the Iris dataset and an SVM involves splitting the data into 5 parts, training and testing the model 5 times, and then averaging the results to get a reliable measure of model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.95833333 0.83333333 1.         0.95833333]\n",
      "Mean cross-validation score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search for Hyperparameter Tuning** is a method used to find the best settings (or hyperparameters) for a machine learning model. This helps us achieve the best performance from our model.\n",
    "\n",
    "### What is Grid Search?\n",
    "\n",
    "Imagine we have a machine learning model, and we want to fine-tune it to perform better. The model has several settings (called hyperparameters) that we can adjust. Grid Search helps us find the best combination of these settings by trying out all possible options.\n",
    "\n",
    "### How Grid Search Works\n",
    "\n",
    "1. **Define the Hyperparameters**:\n",
    "   - We decide which hyperparameters we want to tune and specify the possible values for each one. This is done using a parameter grid.\n",
    "\n",
    "2. **Create the Grid**:\n",
    "   - Grid Search creates a \"grid\" of all possible combinations of these hyperparameters based on the values we provided.\n",
    "\n",
    "3. **Train and Evaluate**:\n",
    "   - For each combination of hyperparameters, Grid Search trains the model and evaluates its performance using cross-validation. This means it splits the data into parts, trains the model on some parts, and tests it on others.\n",
    "\n",
    "4. **Find the Best Combination**:\n",
    "   - After evaluating all combinations, Grid Search identifies the combination of hyperparameters that gives the best performance according to a chosen metric (like accuracy).\n",
    "\n",
    "### Example with SVM\n",
    "\n",
    "Let’s look at your example with the Support Vector Machine (SVM) model and Grid Search:\n",
    "\n",
    "1. **Parameter Grid**:\n",
    "   - We have three hyperparameters to tune for the SVM model:\n",
    "     - **`C`**: The regularization parameter, which can be `0.1`, `1`, or `10`.\n",
    "     - **`kernel`**: The type of kernel used by the SVM, which can be `'linear'` or `'rbf'`.\n",
    "     - **`gamma`**: A parameter for the `'rbf'` kernel, which can be `'scale'` or `'auto'`.\n",
    "\n",
    "2. **Create Grid Search Object**\n",
    "   - We set up the Grid Search to use the SVM model with the   specified parameter grid, 5-fold cross-validation, and accuracy as the performance metric.\n",
    "\n",
    "3. **Train and Find Best Hyperparameters**\n",
    "    - We fit the Grid Search object to our data. It will train the SVM model for every combination of hyperparameters in the grid, evaluate the model using cross-validation, and find the combination that gives the best accuracy.\n",
    "\n",
    "4. **Get the Best Parameters**\n",
    "    - After fitting, we can get the best combination of hyperparameters from the Grid Search results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best score: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Best Model Using Metrics\n",
    "\n",
    "When we evaluate our machine learning model, we use several metrics to understand how well it performs. Here’s a simple explanation of each metric:\n",
    "\n",
    "1. **Accuracy**\n",
    "   - **What it is**: Measures how often the model makes the correct prediction overall.\n",
    "   - **How to understand it**: If the model correctly identifies 90 out of 100 items, the accuracy is 90%.\n",
    "   - **Formula**:\n",
    "     \\begin{equation*}\n",
    "     \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "     \\end{equation*}\n",
    "   - **Example**: If our model correctly classifies 45 out of 50 Iris flowers, the accuracy is \n",
    "\\begin{equation*}\n",
    "\\text{Accuracy} = \\frac{45}{50} = 0.90 \\text{ or 90\\%}\n",
    "\\end{equation*}\n",
    "\n",
    "2. **Precision**\n",
    "   - **What it is**: Measures how many of the predictions made by the model are actually correct for a specific class.\n",
    "   - **How to understand it**: If the model predicts 10 items as a certain class and 8 are correct, the precision is 80%.\n",
    "   - **Formula**:\n",
    "     \\begin{equation*}\n",
    "     \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "     \\end{equation*}\n",
    "   - **Example**: If our model predicts 5 flowers as \"Iris Setosa\" and 4 are actually \"Iris Setosa,\" the precision is \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Precision} = \\frac{4}{5} = 0.80 \\text{ or 80\\%}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "3. **Recall**\n",
    "   - **What it is**: Measures how many of the actual positive cases were correctly identified by the model.\n",
    "   - **How to understand it**: If there are 10 actual positive cases and 8 were identified, recall is 80%.\n",
    "   - **Formula**:\n",
    "     \\begin{equation*}\n",
    "     \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "     \\end{equation*}\n",
    "   - **Example**: If there are 10 actual \"Iris Setosa\" flowers and the model identifies 6, the recall is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Recall} = \\frac{6}{10} = 0.60 \\text{ or 60\\%}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "4. **F1 Score**\n",
    "   - **What it is**: Combines precision and recall into a single number.\n",
    "   - **How to understand it**: Balances precision and recall to provide a single metric for model performance.\n",
    "   - **Formula**:\n",
    "     \\begin{equation*}\n",
    "     \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "     \\end{equation*}\n",
    "   - **Example**: With precision of 80% and recall of 60%, the F1 Score is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{F1 Score} = 2 \\times \\frac{0.80 \\times 0.60}{0.80 + 0.60} = 0.68 \\text{ or 68\\%}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "- **Accuracy** tells us how often the model is correct overall.\n",
    "- **Precision** tells us how accurate the model is for a specific class.\n",
    "- **Recall** tells us how well the model finds all instances of a class.\n",
    "- **F1 Score** combines precision and recall into one metric for a balanced view of performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
