{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble methods** combine the predictions of multiple models to improve the overall performance of a model. Instead of relying on a single model, ensembles utilize multiple models (often weak learners) and aggregate their predictions to achieve a more accurate or stable result.\n",
    "\n",
    "The main ensemble methods are:\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating)**\n",
    "- **Boosting**\n",
    "- **Stacking**\n",
    "\n",
    "\n",
    "Ensemble methods improve the performance of weak models by combining them into a stronger, more robust model. By aggregating the predictions from multiple models, the overall performance is enhanced.\n",
    "\n",
    "### Common Applications\n",
    "- **Classification**\n",
    "- **Regression**\n",
    "\n",
    "### Why Ensemble Methods?\n",
    "\n",
    "- **Reduce overfitting and variance**\n",
    "- **Improve model performance**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Bagging** (Bootstrap Aggregating) is an ensemble method that trains multiple models (typically of the same type) independently on different subsets of the data, which are created via bootstrapping. The final prediction is made by averaging the predictions for regression tasks or taking a vote for classification tasks.\n",
    "\n",
    "Bagging helps to **reduce variance** and is particularly effective with high-variance models, such as decision trees.\n",
    "\n",
    "### Steps of Bagging:\n",
    "\n",
    "1. **Generate multiple subsets** of the training dataset using random sampling with replacement (bootstrapping).\n",
    "2. **Train the same model** on each subset.\n",
    "3. **For classification**: Aggregate the results using a majority vote.\n",
    "   **For regression**: Take the average of the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "base_model = DecisionTreeClassifier()\n",
    "\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=50, random_state=42)\n",
    "\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Bagging Classifier Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Boosting** is an ensemble technique where models are trained sequentially, with each new model focusing on the mistakes made by previous models. The core idea is to give more weight to misclassified points in subsequent iterations, improving the overall model's performance.\n",
    "\n",
    "### Popular Boosting Algorithms:\n",
    "- **AdaBoost** (Adaptive Boosting)\n",
    "- **Gradient Boosting**\n",
    "- **XGBoost**\n",
    "- **LightGBM**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Classifier Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "boosting_model = AdaBoostClassifier(base_model, n_estimators=50, random_state=42)\n",
    "\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = boosting_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Boosting Classifier Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
